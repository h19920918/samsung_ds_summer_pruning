{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OlcM4s6j4ZkZ"
   },
   "outputs": [],
   "source": [
    "# 데이터셋과 학습된 모델을 저장할 폴더 생성\n",
    "!mkdir images\n",
    "!mkdir models\n",
    "!mkdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPCAyWlg4kPi"
   },
   "outputs": [],
   "source": [
    "# ! pip install -U tensorflow\n",
    "# ! pip install tensorflow-model-optimization\n",
    "# ! pip install --upgrade grpcio\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as axes\n",
    "import numpy as np\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "import tensorboard\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "import zipfile\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "from numpy import linalg as LA\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "\n",
    "# Tensorflow 버전 확인\n",
    "print(tf.__version__)\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context('notebook')\n",
    "pd.set_option('display.max_rows', 30)\n",
    "np.random.seed(1337)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7OFC2Vt5Zrt"
   },
   "outputs": [],
   "source": [
    "# MNIST와 Fashion-MNIST 데이터셋 다운로드 / 불러오기 함수\n",
    "def load_dataset(dataset='mnist'):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the data for this task.\n",
    "    Args:\n",
    "      dataset: the name of the dataset to be used for this classification task.\n",
    "        (mnist | fmnist)\n",
    "    Returns:\n",
    "      x_train: Features for training data \n",
    "      x_test: Features for test data\n",
    "      y_train: Labels for training data\n",
    "      y_test: Labels for test data\n",
    "      num_classes: Number of classes for the dataset\n",
    "    \"\"\"\n",
    "    # 학습 이미지 사이즈 (28, 28, 1) / MNIST와 Fashion-MNIST 이미지 사이즈는 동일\n",
    "    img_rows, img_cols = 28, 28\n",
    "    \n",
    "    if dataset=='mnist':\n",
    "        # MNIST 데이터셋 클래스 갯수\n",
    "        num_classes = 10\n",
    "        \n",
    "        # MNIST 데이터셋 불러오기 / Training과 Test 셋 분리 후 셔플\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    elif dataset=='fmnist':\n",
    "        # Fashion-MNIST 데이터셋 클래스 갯수\n",
    "        num_classes = 10\n",
    "        \n",
    "        # Fashion-MNIST 데이터셋 불러오기 / Training과 Test 셋 분리 후 셔플\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "        \n",
    "    else:\n",
    "        print('dataset name does not match available options \\n( mnist | keras )')\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows*img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows*img_cols)\n",
    "    input_shape = (img_rows*img_cols*1,)\n",
    "\n",
    "    # 이미지 타입 전환 / 정규화\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    # 데이터셋 클래스를 Vector로 전환\n",
    "    # e.g. [0, 1, 2] --> [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, num_classes, input_shape\n",
    "\n",
    "\n",
    "# MNIST 데이터셋 정의\n",
    "mnist_x_train, mnist_x_test, mnist_y_train, mnist_y_test, num_classes, input_shape = load_dataset(dataset='mnist')\n",
    "\n",
    "# Fashion-MNIST 데이터셋 정의\n",
    "fmnist_x_train, fmnist_x_test, fmnist_y_train, fmnist_y_test, num_classes, input_shape = load_dataset(dataset='fmnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rugKjW-G5cYa"
   },
   "outputs": [],
   "source": [
    "# 계층 모듈 정의 (e.g. Fully-connected layer, CNN, RNN etc.)\n",
    "l = tf.keras.layers\n",
    "\n",
    "# 심층 학습 모델 빌드 함수\n",
    "def build_model_arch(input_shape, num_classes, sparsity=0.0):\n",
    "    \"\"\"\n",
    "    Builds the model architecture\n",
    "    Args:\n",
    "      input_shape: The tuple describing the input shape\n",
    "      num_classes: how many classes the data labels belong to\n",
    "      sparsity: For compressing already sparse models, how much sparsity was used\n",
    "    Returns:\n",
    "      model: an un-compiled TF.Keras model with 4 hidden\n",
    "        dense layers with shapes [1000, 1000, 500, 200]      \n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # 5 Fully-connected layer with ReLU function 정의\n",
    "    # 한개의 클래스의 Probability를 최대화 하기위해서 마지막 단에 Softmax function 사용\n",
    "    model.add(l.Dense(int(1000-(1000*sparsity)), activation='relu',\n",
    "                      input_shape=input_shape)),\n",
    "    model.add(l.Dense(int(1000-(1000*sparsity)), activation='relu'))\n",
    "    model.add(l.Dense(int(500-(500*sparsity)), activation='relu'))\n",
    "    model.add(l.Dense(int(200-(200*sparsity)), activation='relu'))\n",
    "    model.add(l.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# MNIST와 Fashion-MNIST 모델 빌드 (Sparsity == 0.0)\n",
    "mnist_model_base = build_model_arch(input_shape, num_classes)\n",
    "fmnist_model_base = build_model_arch(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1U6G3PQ5ejy"
   },
   "outputs": [],
   "source": [
    "# Log 파일 생성\n",
    "logdir = './logs'\n",
    "print('Writing training logs to ' + logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSQbA8v65idl"
   },
   "outputs": [],
   "source": [
    "# 심층 학습 모델 학습 함수\n",
    "def make_nosparse_model(model, x_train, y_train, batch_size, epochs, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Training our original model, pre-pruning\n",
    "    Args:\n",
    "      model: Uncompiled Keras model\n",
    "      x_train: Features for training data \n",
    "      y_train: Labels for training data\n",
    "      batch_size: Batch size for training\n",
    "      epochs: Number of epochs for training\n",
    "      x_test: Features for test data\n",
    "      y_test: Labels for test data\n",
    "    Returns:\n",
    "      model: compiled model\n",
    "      score: List of both final test loss and final test accuracy\n",
    "    \"\"\"\n",
    "    callbacks = [tf.keras.callbacks.TensorBoard(log_dir=logdir, profile_batch=0)]\n",
    "\n",
    "    # 심층 학습 모델 컴파일 \n",
    "    # Loss function: Cross entropy loss\n",
    "    # Optimizer: Adam optimizer\n",
    "    # Measure: Accuracy\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.categorical_crossentropy,\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    # 심층 학습 모델 학습\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=(x_test, y_test))\n",
    "    \n",
    "    # 심층 학습 모델 평가\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    return model, score\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "\n",
    "mnist_model, mnist_score = make_nosparse_model(mnist_model_base,\n",
    "                                               mnist_x_train,\n",
    "                                               mnist_y_train,\n",
    "                                               batch_size,\n",
    "                                               epochs,\n",
    "                                               mnist_x_test,\n",
    "                                               mnist_y_test)\n",
    "print(mnist_model.summary())\n",
    "\n",
    "fmnist_model, fmnist_score = make_nosparse_model(fmnist_model_base,\n",
    "                                                 fmnist_x_train,\n",
    "                                                 fmnist_y_train,\n",
    "                                                 batch_size,\n",
    "                                                 epochs,\n",
    "                                                 fmnist_x_test,\n",
    "                                                 fmnist_y_test)\n",
    "\n",
    "print(fmnist_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J-q-2ZXI5pPg"
   },
   "outputs": [],
   "source": [
    "# Tensorboard 실행\n",
    "%tensorboard --logdir={logdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QsP5v-ok5w9Y"
   },
   "outputs": [],
   "source": [
    "# Pruning 함수 (Weight)\n",
    "def weight_prune_dense_layer(k_weights, b_weights, k_sparsity):\n",
    "    \"\"\"\n",
    "    Takes in matrices of kernel and bias weights (for a dense\n",
    "      layer) and returns the unit-pruned versions of each\n",
    "    Args:\n",
    "      k_weights: 2D matrix of the \n",
    "      b_weights: 1D matrix of the biases of a dense layer\n",
    "      k_sparsity: percentage of weights to set to 0\n",
    "    Returns:\n",
    "      kernel_weights: sparse matrix with same shape as the original\n",
    "        kernel weight matrix\n",
    "      bias_weights: sparse array with same shape as the original\n",
    "        bias array\n",
    "    \"\"\"\n",
    "\n",
    "    # Kernel weights 복사\n",
    "    kernel_weights = np.copy(k_weights)\n",
    "\n",
    "    # Weight의 절대값을 기준으로 인덱스 정렬 (2D martix)\n",
    "    ind = np.unravel_index(\n",
    "        np.argsort(\n",
    "            np.abs(kernel_weights),\n",
    "            axis=None),\n",
    "        kernel_weights.shape)\n",
    "        \n",
    "    # Pruning 하는 Weights 갯수 정의\n",
    "    cutoff = int(len(ind[0])*k_sparsity)\n",
    "\n",
    "    # 정렬된 인덱스를 기준으로 Weight pruning 수행 (0)\n",
    "    sparse_cutoff_inds = (ind[0][0:cutoff], ind[1][0:cutoff])\n",
    "    kernel_weights[sparse_cutoff_inds] = 0.\n",
    "        \n",
    "    # Kernel bias에 대하여 동일한 작업 수행\n",
    "    bias_weights = np.copy(b_weights)\n",
    "\n",
    "    ind = np.unravel_index(\n",
    "        np.argsort(\n",
    "            np.abs(bias_weights), \n",
    "            axis=None), \n",
    "        bias_weights.shape)\n",
    "        \n",
    "    cutoff = int(len(ind[0])*k_sparsity)\n",
    "\n",
    "    sparse_cutoff_inds = (ind[0][0:cutoff])\n",
    "    bias_weights[sparse_cutoff_inds] = 0.\n",
    "    \n",
    "    return kernel_weights, bias_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJh2RWpf3cYh"
   },
   "outputs": [],
   "source": [
    "# Pruning 함수 (Unit)\n",
    "def unit_prune_dense_layer(k_weights, b_weights, k_sparsity):\n",
    "    \"\"\"\n",
    "    Takes in matrices of kernel and bias weights (for a dense\n",
    "      layer) and returns the unit-pruned versions of each\n",
    "    Args:\n",
    "      k_weights: 2D matrix of the \n",
    "      b_weights: 1D matrix of the biases of a dense layer\n",
    "      k_sparsity: percentage of weights to set to 0\n",
    "    Returns:\n",
    "      kernel_weights: sparse matrix with same shape as the original\n",
    "        kernel weight matrix\n",
    "      bias_weights: sparse array with same shape as the original\n",
    "        bias array\n",
    "    \"\"\"\n",
    "\n",
    "    # Kernel weights 복사\n",
    "\n",
    "    # Column-wise L2 Norms 계산 후 인덱스 정렬\n",
    "\n",
    "    # Pruning 하는 Weights 갯수 정의\n",
    "\n",
    "    # 정렬된 인덱스를 기준으로 Weight pruning 수행 (0)\n",
    "\n",
    "    # Kernel bias에 대하여 Weight를 기준으로 Pruning 된 인덱스에 대하여 0 적용\n",
    "\n",
    "    return kernel_weights, bias_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GuO1jtHw51_c"
   },
   "outputs": [],
   "source": [
    "def sparsify_model(model, x_test, y_test, k_sparsity, pruning='weight'):\n",
    "    \"\"\"\n",
    "    Takes in a model made of dense layers and prunes the weights\n",
    "    Args:\n",
    "      model: Keras model\n",
    "      k_sparsity: target sparsity of the model\n",
    "    Returns:\n",
    "      sparse_model: sparsified copy of the previous model\n",
    "    \"\"\"\n",
    "    # 원본 모델 파라미터 복사\n",
    "    sparse_model = tf.keras.models.clone_model(model)\n",
    "    sparse_model.set_weights(model.get_weights())\n",
    "    \n",
    "    # 모델 변수명 & 파라미터 값 정의\n",
    "    names = [weight.name for layer in sparse_model.layers for weight in layer.weights]\n",
    "    weights = sparse_model.get_weights()\n",
    "    \n",
    "    # Pruning이 적용된 파라미터를 저장하기 위한 공간\n",
    "    newWeightList = []\n",
    "\n",
    "    # 마지막 2개의 Layer를 제외한 나머지 Layer에 대하여 Pruning 수행\n",
    "    # 마지막 2개의 Layer는 Logit 생성과 Softmax 함수 [(200, 10,), (10,)]\n",
    "    for i in range(0, len(weights)-2, 2):\n",
    "        \n",
    "        if pruning=='weight':\n",
    "            kernel_weights, bias_weights = weight_prune_dense_layer(weights[i],\n",
    "                                                                    weights[i+1],\n",
    "                                                                    k_sparsity)\n",
    "        elif pruning=='unit':\n",
    "            kernel_weights, bias_weights = unit_prune_dense_layer(weights[i],\n",
    "                                                                  weights[i+1],\n",
    "                                                                  k_sparsity)\n",
    "        else:\n",
    "            print('does not match available pruning methods ( weight | unit )')\n",
    "        \n",
    "        # Pruning 된 Weights와 Bias 저장\n",
    "        newWeightList.append(kernel_weights)\n",
    "        newWeightList.append(bias_weights)\n",
    "\n",
    "    # 마지막 2개의 Layer는 값이 변하지 않으므로 원본값 추가\n",
    "    for i in range(len(weights)-2, len(weights)):\n",
    "        unmodified_weight = np.copy(weights[i])\n",
    "        newWeightList.append(unmodified_weight)\n",
    "\n",
    "    # Pruning 된 파라미터를 Set\n",
    "    sparse_model.set_weights(newWeightList)\n",
    "    \n",
    "    # 심층 학습 모델 Re-compile\n",
    "    sparse_model.compile(\n",
    "        loss=tf.keras.losses.categorical_crossentropy,\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    # Pruning %에 의한 Loss와 Accuracy 출력\n",
    "    score = sparse_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('k% weight sparsity: ', k_sparsity,\n",
    "          '\\tTest loss: {:07.5f}'.format(score[0]),\n",
    "          '\\tTest accuracy: {:05.2f} %%'.format(score[1]*100.))\n",
    "    \n",
    "    return sparse_model, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_EVa_Fn53z8"
   },
   "outputs": [],
   "source": [
    "# Pruning 비율 설정\n",
    "k_sparsities = [0.0, 0.25, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.97, 0.99]\n",
    "\n",
    "# 초기값 정의\n",
    "mnist_model_loss_weight = []\n",
    "mnist_model_accs_weight = []\n",
    "# mnist_model_loss_unit = []\n",
    "# mnist_model_accs_unit = []\n",
    "fmnist_model_loss_weight = []\n",
    "fmnist_model_accs_weight = []\n",
    "# fmnist_model_loss_unit = []\n",
    "# fmnist_model_accs_unit = []\n",
    "\n",
    "# MNIST 데이터셋을 학습한 심층 학습 모델에 대하여 Pruning 수행\n",
    "dataset = 'mnist'\n",
    "pruning = 'weight'\n",
    "print('\\n MNIST Weight-pruning\\n')\n",
    "for k_sparsity in k_sparsities:\n",
    "    sparse_model, score = sparsify_model(mnist_model, x_test=mnist_x_test,\n",
    "                                         y_test=mnist_y_test,\n",
    "                                         k_sparsity=k_sparsity, \n",
    "                                         pruning=pruning)\n",
    "    mnist_model_loss_weight.append(score[0])\n",
    "    mnist_model_accs_weight.append(score[1])\n",
    "    \n",
    "    sparse_model.save('models/sparse_{}-model_k-{}_{}-pruned.h5'.format(dataset, k_sparsity, pruning))\n",
    "    del sparse_model\n",
    "\n",
    "\n",
    "# pruning='unit'\n",
    "# print('\\n MNIST Unit-pruning\\n')\n",
    "# for k_sparsity in k_sparsities:\n",
    "#     sparse_model, score = sparsify_model(mnist_model, x_test=mnist_x_test,\n",
    "#                                          y_test=mnist_y_test, \n",
    "#                                          k_sparsity=k_sparsity, \n",
    "#                                          pruning=pruning)\n",
    "#     mnist_model_loss_unit.append(score[0])\n",
    "#     mnist_model_accs_unit.append(score[1])\n",
    "#     \n",
    "#     sparse_model.save('models/sparse_{}-model_k-{}_{}-pruned.h5'.format(dataset, k_sparsity, pruning))\n",
    "#     del sparse_model\n",
    "\n",
    "\n",
    "# Fashion-MNIST 데이터셋을 학습한 심층 학습 모델에 대하여 Pruning 수행\n",
    "dataset = 'fmnist'\n",
    "pruning = 'weight'\n",
    "print('\\n FMNIST Weight-pruning\\n')\n",
    "for k_sparsity in k_sparsities:\n",
    "    sparse_model, score = sparsify_model(fmnist_model, x_test=fmnist_x_test,\n",
    "                                         y_test=fmnist_y_test,\n",
    "                                         k_sparsity=k_sparsity, \n",
    "                                         pruning=pruning)\n",
    "    fmnist_model_loss_weight.append(score[0])\n",
    "    fmnist_model_accs_weight.append(score[1])\n",
    "    \n",
    "    sparse_model.save('models/sparse_{}-model_k-{}_{}-pruned.h5'.format(dataset, k_sparsity, pruning))\n",
    "    del sparse_model\n",
    "\n",
    "\n",
    "# pruning='unit'\n",
    "# print('\\n FMNIST Unit-pruning\\n')\n",
    "# for k_sparsity in k_sparsities:\n",
    "#     sparse_model, score = sparsify_model(fmnist_model, x_test=fmnist_x_test,\n",
    "#                                          y_test=fmnist_y_test, \n",
    "#                                          k_sparsity=k_sparsity, \n",
    "#                                          pruning=pruning)\n",
    "#     fmnist_model_loss_unit.append(score[0])\n",
    "#     fmnist_model_accs_unit.append(score[1])\n",
    "#     \n",
    "#     sparse_model.save('models/sparse_{}-model_k-{}_{}-pruned.h5'.format(dataset, k_sparsity, pruning))\n",
    "#     del sparse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIxAuzCS7hf6"
   },
   "outputs": [],
   "source": [
    "# 리스트 타입을 Numpy array로 변환\n",
    "k_sparsities = np.asarray(k_sparsities)\n",
    "mnist_model_loss_weight = np.asarray(mnist_model_loss_weight)\n",
    "mnist_model_accs_weight = np.asarray(mnist_model_accs_weight)\n",
    "# mnist_model_loss_unit = np.asarray(mnist_model_loss_unit)\n",
    "# mnist_model_accs_unit = np.asarray(mnist_model_accs_unit)\n",
    "fmnist_model_loss_weight = np.asarray(fmnist_model_loss_weight)\n",
    "fmnist_model_accs_weight = np.asarray(fmnist_model_accs_weight)\n",
    "# fmnist_model_loss_unit = np.asarray(fmnist_model_loss_unit)\n",
    "# fmnist_model_accs_unit = np.asarray(fmnist_model_accs_unit)\n",
    "\n",
    "# DataFrame을 사용하기 위해서 각 Array stacking\n",
    "# sparsity_data = np.stack([k_sparsities,\n",
    "#                           mnist_model_loss_weight,\n",
    "#                           mnist_model_accs_weight,\n",
    "#                           mnist_model_loss_unit,\n",
    "#                           mnist_model_accs_unit,\n",
    "#                           fmnist_model_loss_weight,\n",
    "#                           fmnist_model_accs_weight,\n",
    "#                           fmnist_model_loss_unit,\n",
    "#                           fmnist_model_accs_unit])\n",
    "\n",
    "sparsity_data = np.stack([k_sparsities,\n",
    "                          mnist_model_loss_weight,\n",
    "                          mnist_model_accs_weight,\n",
    "                          fmnist_model_loss_weight,\n",
    "                          fmnist_model_accs_weight])\n",
    "\n",
    "\n",
    "# Pandas DataFrame 정의\n",
    "# sparsity_summary = pd.DataFrame(data=sparsity_data.T,    # 값\n",
    "#                                 columns=['k_sparsity',   # 열 변수명\n",
    "#                                          'mnist_loss_weight',\n",
    "#                                          'mnist_acc_weight',\n",
    "#                                          'mnist_loss_unit',\n",
    "#                                          'mnist_acc_unit',\n",
    "#                                          'fmnist_loss_weight',\n",
    "#                                          'fmnist_acc_weight',\n",
    "#                                          'fmnist_loss_unit',\n",
    "#                                          'fmnist_acc_unit'])\n",
    "\n",
    "sparsity_summary = pd.DataFrame(data=sparsity_data.T,    # 값\n",
    "                                columns=['k_sparsity',   # 열 변수명\n",
    "                                         'mnist_loss_weight',\n",
    "                                         'mnist_acc_weight',\n",
    "                                         'fmnist_loss_weight',\n",
    "                                         'fmnist_acc_weight'])\n",
    "\n",
    "sparsity_summary.to_csv('sparsity_summary.csv')\n",
    "sparsity_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z7niMbNo7kKt"
   },
   "outputs": [],
   "source": [
    "# 모델 파라미터 Visulization\n",
    "def visualize_model_weights(sparse_model):\n",
    "    \"\"\"\n",
    "    Visualize the weights of the layers of the sparse model.\n",
    "    For weights with values of 0, they will be represented by the color white\n",
    "    Args:\n",
    "      sparse_model: a TF.Keras model\n",
    "    \"\"\"\n",
    "    weights = sparse_model.get_weights()\n",
    "    names = [weight.name for layer in sparse_model.layers for weight in layer.weights]\n",
    "    \n",
    "    my_cmap = matplotlib.cm.get_cmap('rainbow')\n",
    "    my_cmap.set_under('w')\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        weight_matrix = weights[i]\n",
    "        layer_name = names[i]\n",
    "        if weight_matrix.ndim == 1:\n",
    "            weight_matrix = np.resize(weight_matrix,\n",
    "                                      (1,weight_matrix.size))\n",
    "            plt.imshow(np.abs(weight_matrix),\n",
    "                       interpolation='none',\n",
    "                       aspect = \"auto\",\n",
    "                       cmap=my_cmap,\n",
    "                       vmin=1e-26);\n",
    "            plt.colorbar()\n",
    "            plt.title(layer_name)\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.imshow(np.abs(weight_matrix),\n",
    "                       interpolation='none',\n",
    "                       cmap=my_cmap,\n",
    "                       vmin=1e-26);\n",
    "            plt.colorbar()\n",
    "            plt.title(layer_name)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxpMVc_z7ofq"
   },
   "outputs": [],
   "source": [
    "#@title Dense Layer visualization { run: \"auto\" }\n",
    "\n",
    "#@markdown In Google Colab, this file becomes interactive, and you can select the sparse model you want to retrieve.\n",
    "#@markdown \n",
    "#@markdown All weights with values of `0.0` will be color-coded weight. 1D Bias layers will be auto-scaled to the dimensions of the 2D plots.\n",
    "#@markdown \n",
    "#@markdown Which dataset?\n",
    "dataset = 'mnist' #@param ['mnist', 'fmnist']\n",
    "#@markdown k sparsity\n",
    "sparsity = \"0.5\" #@param ['0.0', '0.25', '0.5', '0.6', '0.7', '0.8', '0.9', '0.95', '0.97', '0.99']\n",
    "#@markdown Pruning method.\n",
    "pruning = 'weight' #@param ['unit', 'weight']\n",
    "\n",
    "sparse_model = load_model('models/sparse_{}-model_k-{}_{}-pruned.h5'.format(dataset, sparsity, pruning))\n",
    "\n",
    "visualize_model_weights(sparse_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-v1Bz2Z8SAj"
   },
   "outputs": [],
   "source": [
    "# MNIST 데이터셋 성능 Visualization\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "plt.grid(b=None)\n",
    "ax2 = ax1.twinx()\n",
    "plt.grid(b=None)\n",
    "plt.title('Test Accuracy as a function of k% Sparsity\\nfor 4-hidden-layer MLP trained on MNIST')\n",
    "ax1.plot(sparsity_summary['k_sparsity'].values,\n",
    "         sparsity_summary['mnist_acc_weight'].values,\n",
    "         '#008fd5', linestyle=':', label='Weight-pruning Acc')\n",
    "# ax1.plot(sparsity_summary['k_sparsity'].values,\n",
    "#          sparsity_summary['mnist_acc_unit'].values,\n",
    "#          '#008fd5', linestyle='-', label='Unit-pruning Acc')\n",
    "ax2.plot(sparsity_summary['k_sparsity'].values,\n",
    "         sparsity_summary['mnist_loss_weight'].values,\n",
    "         '#fc4f30', linestyle=':', label='Weight-pruning Loss')\n",
    "# ax2.plot(sparsity_summary['k_sparsity'].values,\n",
    "#          sparsity_summary['mnist_loss_unit'].values,\n",
    "#          '#fc4f30', linestyle='-', label='Unit-pruning Loss')\n",
    "\n",
    "ax1.set_ylabel('Accuracy (%)', color='#008fd5')\n",
    "ax2.set_ylabel('Loss (categorical crossentropy)', color='#fc4f30')\n",
    "ax1.set_xlabel('k% Sparsity')\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), shadow=True, ncol=2);\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.25), shadow=True, ncol=2);\n",
    "plt.savefig('images/MNIST_sparsity_comparisons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe9qL5LS8Tgk"
   },
   "outputs": [],
   "source": [
    "# Fashion-MNIST 데이터셋 성능 Visualization\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "plt.grid(b=None)\n",
    "ax2 = ax1.twinx()\n",
    "plt.grid(b=None)\n",
    "plt.title('Test Accuracy as a function of k% Sparsity\\nfor 4-hidden-layer MLP trained on FMNIST')\n",
    "ax1.plot(sparsity_summary['k_sparsity'].values,\n",
    "         sparsity_summary['fmnist_acc_weight'].values,\n",
    "         '#008fd5', linestyle=':', label='Weight-pruning Acc')\n",
    "# ax1.plot(sparsity_summary['k_sparsity'].values,\n",
    "#          sparsity_summary['fmnist_acc_unit'].values,\n",
    "#          '#008fd5', linestyle='-', label='Unit-pruning Acc')\n",
    "ax2.plot(sparsity_summary['k_sparsity'].values,\n",
    "         sparsity_summary['fmnist_loss_weight'].values,\n",
    "         '#fc4f30', linestyle=':', label='Weight-pruning Loss')\n",
    "# ax2.plot(sparsity_summary['k_sparsity'].values,\n",
    "#          sparsity_summary['fmnist_loss_unit'].values,\n",
    "#          '#fc4f30', linestyle='-', label='Unit-pruning Loss')\n",
    "\n",
    "ax1.set_ylabel('Accuracy (%)', color='#008fd5')\n",
    "ax2.set_ylabel('Loss (categorical crossentropy)', color='#fc4f30')\n",
    "ax1.set_xlabel('k% Sparsity')\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), shadow=True, ncol=2);\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.25), shadow=True, ncol=2);\n",
    "plt.savefig('images/FMNIST_sparsity_comparisons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pruning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
